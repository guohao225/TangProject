{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "cur_dir = './Training'\n",
    "def get_file_list(ftype):\n",
    "    txt_list = []\n",
    "    for file_name in os.listdir(cur_dir):\n",
    "        if file_name.endswith(ftype):\n",
    "            num = file_name.split('.')\n",
    "            txt_list.append({'file':cur_dir+'/'+file_name, 'value':num[0]})\n",
    "    return txt_list\n",
    "\n",
    "def gen_txt_obj(file_name):\n",
    "    file_obj = []\n",
    "    begin = 0\n",
    "    end = 0\n",
    "    f = open(file_name, 'r', encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        begin = end\n",
    "        end += len(line)\n",
    "        file_obj.append({'text':line, 'begin':begin, 'end':end})\n",
    "    f.close()\n",
    "    return file_obj\n",
    "\n",
    "txt_list = get_file_list('.txt')\n",
    "txt_all = {}\n",
    "for txt in txt_list:\n",
    "    temp = gen_txt_obj(txt['file'])\n",
    "    txt_all[txt['value']] = temp\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def gen_ann_obj(file_name):\n",
    "    f = open(file_name, 'r', encoding='utf8')\n",
    "    lines = f.readlines()\n",
    "    entitys = {}\n",
    "    relations = {}\n",
    "    for line in lines:\n",
    "        if line.startswith('T'):\n",
    "            line = line.strip()\n",
    "            ##以空格拆分行\n",
    "            info = re.split('[\\t ]', line)\n",
    "            ##删除info中存在;的元素\n",
    "            info = [item for item in info if \";\" not in item]\n",
    "            entitys[info[0]] = {'text':info[4] if len(info) == 5 else \"\\n\".join(info[4:len(info)]),'begin':info[2], 'end':info[3]}\n",
    "        elif line.startswith('R'):\n",
    "            line = line.strip()\n",
    "            info = re.split('[\\t ]', line)\n",
    "            relations[info[0]] = {\n",
    "                'relation':info[1],\n",
    "                'h':{'name':info[2].split(':')[1]},\n",
    "                't':{'name':info[3].split(':')[1]}\n",
    "            }\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('*'):\n",
    "            line = line.strip()\n",
    "            info = re.split('[\\t ]', line)\n",
    "            coref = info[2:len(info)]\n",
    "            relation_id = 0\n",
    "            for i in range(len(coref)):\n",
    "                for j in range(i+1, len(coref)):\n",
    "                    if int(entitys[coref[j]]['end']) - int(entitys[coref[i]]['end'])  < 100:\n",
    "                        relations[\"*\"+str(relation_id)] = {\n",
    "                            'relation':'coreference',\n",
    "                            'h':{'name':coref[i]},\n",
    "                            't':{'name':coref[j]}\n",
    "                        }\n",
    "                        relation_id += 1\n",
    "\n",
    "    relation_list = []\n",
    "    for item in relations:\n",
    "        data = relations[item]\n",
    "        begin_h = entitys[data['h']['name']]['begin']\n",
    "        begin_t = entitys[data['t']['name']]['begin']\n",
    "        end_h = entitys[data['h']['name']]['end']\n",
    "        end_t = entitys[data['t']['name']]['end']\n",
    "        name_h = entitys[data['h']['name']]['text']\n",
    "        name_t = entitys[data['t']['name']]['text']\n",
    "        data['h'] = {'name':name_h, 'begin':int(begin_h), 'end':int(end_h)}\n",
    "        data['t'] = {'name':name_t, 'begin':int(begin_t), 'end':int(end_t)}\n",
    "        relation_list.append(relations[item])\n",
    "    return relation_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "## 处理位置信息\n",
    "def merge_location(file_index):\n",
    "    text_data = txt_all[file_index]\n",
    "    relations = gen_ann_obj(f'{cur_dir}/{file_index}.ann')\n",
    "    ab_texts = open(f'{cur_dir}/{file_index}.txt', 'r', encoding='utf8')\n",
    "    ab_texts = \"\".join(ab_texts.readlines())\n",
    "    data = []\n",
    "    for item in relations:\n",
    "        begin = item['t']['begin'] if item['h']['begin'] > item['t']['begin'] else item['h']['begin']\n",
    "        end = item['t']['end'] if item['h']['end'] < item['t']['end'] else item['h']['end']\n",
    "        for i in range(len(text_data)):\n",
    "            if text_data[i]['begin'] <= begin < text_data[i]['end']:\n",
    "                text = text_data[i]['text']\n",
    "                if text_data[i]['end'] < end:\n",
    "                    text = ab_texts[text_data[i]['begin']:end]\n",
    "                    # print(ntext+'\\n'+text+\"\\n\")\n",
    "                ## 计算实体在句子中的位置\n",
    "                item['h']['pos'] = [item['h']['begin'] - text_data[i]['begin'], item['h']['end']-text_data[i]['begin']]\n",
    "                item['t']['pos'] = [item['t']['begin'] - text_data[i]['begin'], item['t']['end'] - text_data[i]['begin']]\n",
    "                item['enty'] = [text[item['h']['pos'][0]:item['h']['pos'][1]],\n",
    "                                text[item['t']['pos'][0]:item['t']['pos'][1]]]\n",
    "                item['text'] = text\n",
    "                data.append(item)\n",
    "    error = 0\n",
    "    for item in data:\n",
    "        if item['h']['name'] != item['enty'][0] or item['t']['name'] != item['enty'][1]:\n",
    "            entity1 = item['h']['name']\n",
    "            entity1_index = item['text'].find(entity1)\n",
    "            item['h']['pos'] = [entity1_index, entity1_index+len(entity1)]\n",
    "            entity2 = item['t']['name']\n",
    "            entity2_index = item['text'].find(entity2)\n",
    "            item['t']['pos'] = [entity2_index,entity2_index+len(entity2)]\n",
    "            item['enty'] = [item['text'][item['h']['pos'][0]:item['h']['pos'][1]],\n",
    "                                item['text'][item['t']['pos'][0]:item['t']['pos'][1]]]\n",
    "            error += 1\n",
    "        # if item['h']['name'] != item['enty'][0] or item['t']['name'] != item['enty'][1]:\n",
    "        #     data.remove(item)\n",
    "        del item['enty']\n",
    "        del item['h']['begin']\n",
    "        del item['h']['end']\n",
    "        del item['t']['begin']\n",
    "        del item['t']['end']\n",
    "    if error > 1:\n",
    "        print(f\"超过1个错误的文件:{file_index}, 错误个数为{error}, 总数为{len(data)}\")\n",
    "    # print(f\"文件{file_index}.ann错误的个数为:{error},总数为{len(data)}\")\n",
    "    return  data, error, len(relations)\n",
    "\n",
    "# merge_location('3')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "超过1个错误的文件:1122, 错误个数为19, 总数为30\n",
      "超过1个错误的文件:293, 错误个数为188, 总数为192\n",
      "超过1个错误的文件:329, 错误个数为2, 总数为55\n",
      "超过1个错误的文件:624, 错误个数为2, 总数为11\n",
      "超过1个错误的文件:669, 错误个数为3, 总数为61\n",
      "超过1个错误的文件:706, 错误个数为15, 总数为28\n",
      "超过1个错误的文件:893, 错误个数为47, 总数为94\n",
      "超过1个错误的文件:898, 错误个数为86, 总数为90\n",
      "超过1个错误的文件:905, 错误个数为7, 总数为15\n",
      "超过1个错误的文件:907, 错误个数为32, 总数为35\n",
      "超过1个错误的文件:993, 错误个数为3, 总数为27\n",
      "18334:18334, 429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18334/18334 [00:00<00:00, 97782.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over_len:754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "file_list = get_file_list('.txt')\n",
    "processed_data = []\n",
    "error = 0\n",
    "relations = 0\n",
    "for one_file in file_list:\n",
    "    data, err,res = merge_location(one_file['value'])\n",
    "    processed_data += data\n",
    "    error += err\n",
    "    relations += res\n",
    "print(f\"{len(processed_data)}:{relations}, {error}\")\n",
    "over_len = 0\n",
    "with open('./train.txt','w', encoding='utf8') as fp:\n",
    "    for item in tqdm(processed_data):\n",
    "        if len(item['text']) > 128:\n",
    "            begin = item['h']['pos'][0] if item['h']['pos'][0] < item['t']['pos'][0] else item['t']['pos'][0]\n",
    "            item['h']['pos'] = [item['h']['pos'][0]-begin, item['h']['pos'][1]-begin]\n",
    "            item['t']['pos'] = [item['t']['pos'][0]-begin, item['t']['pos'][1]-begin]\n",
    "            item['text'] = item['text'][begin: len(item['text'])]\n",
    "        if len(item['text']) < 128:\n",
    "            fp.write(json.dumps(item, ensure_ascii=False)+'\\n')\n",
    "        else:\n",
    "            over_len += 1\n",
    "\n",
    "print(f\"over_len:{over_len}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'4'>'14'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
